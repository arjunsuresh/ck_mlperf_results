[
  {
    "Accuracy": 0.8025,
    "Accuracy_Metric": "AUC",
    "Benchmark": "recommendation",
    "Dataset": "1TB Clickthrough",
    "Model_ID": "dlrm",
    "Organization": "Intel",
    "Result": 22.262705555555556,
    "Result_Units": "min.",
    "Task": "recommendation",
    "_Accuracy": 0.8025,
    "_Accuracy_Metric": "AUC",
    "_Dataset": "1TB Clickthrough",
    "_Model_ID": "dlrm",
    "_Organization": "Intel",
    "_Result": 22.262705555555556,
    "_Result_Units": "min.",
    "_System": "4-nodes-32s-8376H-pytorch",
    "_Task": "recommendation",
    "_version": "1.0",
    "accelerator_model_name": "N/A",
    "accelerators_count": 0,
    "availability": "available",
    "code_url": "https://github.com/mlcommons/training_results_v1.0/blob/master/Intel/benchmarks",
    "details_url": "https://github.com/mlcommons/training_results_v1.0/blob/main/Intel/systems/4-nodes-32s-8376H-pytorch.json",
    "division": "open",
    "dlrm": 22.262705555555556,
    "framework": "PyTorch v1.5",
    "git_url": "https://github.com/mlcommons/training_results_v1.0",
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8376H CPU @ 2.60GHz",
    "host_processors_count": 32,
    "notes": "We use hybrid optimizer, LAMB for data-parallel layers and SGD for embedding, for large-batch training",
    "submitter": "Intel",
    "system": "4-nodes-32s-8376H-pytorch",
    "uid": "a29093e5ae454fd6",
    "unet3d": "",
    "version": "1.0"
  },
  {
    "Accuracy": 0.8025,
    "Accuracy_Metric": "AUC",
    "Benchmark": "recommendation",
    "Dataset": "1TB Clickthrough",
    "Model_ID": "dlrm",
    "Organization": "Intel",
    "Result": 14.976555555555555,
    "Result_Units": "min.",
    "Task": "recommendation",
    "_Accuracy": 0.8025,
    "_Accuracy_Metric": "AUC",
    "_Dataset": "1TB Clickthrough",
    "_Model_ID": "dlrm",
    "_Organization": "Intel",
    "_Result": 14.976555555555555,
    "_Result_Units": "min.",
    "_System": "8-nodes-64s-8376H-pytorch",
    "_Task": "recommendation",
    "_version": "1.0",
    "accelerator_model_name": "N/A",
    "accelerators_count": 0,
    "availability": "available",
    "code_url": "https://github.com/mlcommons/training_results_v1.0/blob/master/Intel/benchmarks",
    "details_url": "https://github.com/mlcommons/training_results_v1.0/blob/main/Intel/systems/8-nodes-64s-8376H-pytorch.json",
    "division": "open",
    "dlrm": 14.976555555555555,
    "framework": "PyTorch v1.5",
    "git_url": "https://github.com/mlcommons/training_results_v1.0",
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8376H CPU @ 2.60GHz",
    "host_processors_count": 64,
    "notes": "We use hybrid optimizer, LAMB for data-parallel layers and SGD for embedding, for large-batch training",
    "submitter": "Intel",
    "system": "8-nodes-64s-8376H-pytorch",
    "uid": "117088f5d7ab4d1f",
    "unet3d": "",
    "version": "1.0"
  }
]
